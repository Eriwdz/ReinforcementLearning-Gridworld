{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of states in grid:  16\n",
      "No. of action options in each state: 4\n",
      "Index for actions:\n",
      "0 : up\n",
      "1 : down\n",
      "2 : left\n",
      "3 : right\n",
      "5 : terminal states (stay)\n",
      "7 : wall\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import random\n",
    "#Defining Grid\n",
    "\n",
    "#defining action variables\n",
    "up=0\n",
    "down=1\n",
    "left=2\n",
    "right=3\n",
    "\n",
    "#no. of states and no. of variables\n",
    "noS=4*4\n",
    "noA=4\n",
    "\n",
    "S=range(noS)\n",
    "\n",
    "step_reward=-1    #for every step\n",
    "wall_reward=-10\n",
    "goal_reward=0\n",
    "\n",
    "terminal_state = lambda s: s==15  #first and last state terminal\n",
    "\n",
    "def update_P(wall):\n",
    "    P=dict() #transition probabilities\n",
    "\n",
    "    for s in S:\n",
    "        P[s]=dict()\n",
    "\n",
    "        if (terminal_state(s)):\n",
    "            P[s][up]=(s,1.0,0.0)   # next_state, probability, reward\n",
    "            P[s][down]=(s,1.0,0.0)\n",
    "            P[s][right]=(s,1.0,0.0)\n",
    "            P[s][left]=(s,1.0,0.0)\n",
    "        else:\n",
    "            next_s= s if(s<4) else s-4\n",
    "            if terminal_state(next_s):\n",
    "                P[s][up]=(next_s,1.0,goal_reward)\n",
    "            elif next_s in wall:\n",
    "                P[s][up]=(s,1.0,-10)\n",
    "            else:\n",
    "                P[s][up]=(next_s,1.0,step_reward)\n",
    "\n",
    "            next_s= s if(16-s<=4) else s+4\n",
    "            if terminal_state(next_s):\n",
    "                P[s][down]=(next_s,1.0,goal_reward)\n",
    "            elif next_s in wall:\n",
    "                P[s][down]=(s,1.0,-10)\n",
    "            else:\n",
    "                P[s][down]=(next_s,1.0,step_reward)\n",
    "\n",
    "            next_s= s if((s+1)%4==0) else s+1\n",
    "            if terminal_state(next_s):\n",
    "                P[s][right]=(next_s,1.0,goal_reward)\n",
    "            elif next_s in wall:\n",
    "                P[s][right]=(s,1.0,-10)\n",
    "            else:\n",
    "                P[s][right]=(next_s,1.0,step_reward)\n",
    "\n",
    "            next_s= s if(s%4==0) else s-1\n",
    "            if terminal_state(next_s):\n",
    "                P[s][left]=(next_s,1.0,goal_reward)\n",
    "            elif next_s in wall:\n",
    "                P[s][left]=(s,1.0,-10)\n",
    "            else:\n",
    "                P[s][left]=(next_s,1.0,step_reward)\n",
    "                \n",
    "    return P\n",
    "        \n",
    "#wall=[5,7,9]\n",
    "#P=update_P(wall)\n",
    "        \n",
    "print 'No. of states in grid: ', noS\n",
    "print 'No. of action options in each state:', noA\n",
    "\n",
    "Action_Index=dict()\n",
    "Action_Index[0]='up'\n",
    "Action_Index[1]='down'\n",
    "Action_Index[2]='left'\n",
    "Action_Index[3]='right'\n",
    "Action_Index[5]='terminal states (stay)'\n",
    "Action_Index[7]='wall'\n",
    "\n",
    "print 'Index for actions:'\n",
    "for k,v in Action_Index.items():\n",
    "    print k,\":\",v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def e_greedy(e,Q_s):\n",
    "    x=random.randrange(1,11)\n",
    "    if x<=e*10:\n",
    "        return random.randrange(noA)\n",
    "    else:\n",
    "        return np.argmax(Q_s)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[-2.81964814 -4.55797742 -2.8091894  -2.77308975]\n",
      " [-2.64572124 -2.53299882 -2.6910316  -2.53299871]\n",
      " [-2.38056985 -7.80508427 -2.31210118 -2.19      ]\n",
      " [-1.99134258 -1.7        -2.28084574 -2.02723707]\n",
      " [-2.58990513 -2.52926196 -5.21737441 -2.52896851]\n",
      " [-2.57258985 -2.19       -6.92142885 -8.46064548]\n",
      " [-2.03997537 -1.68286113 -1.93953083 -1.68166331]\n",
      " [-1.9224856  -1.         -9.24408012 -1.5577243 ]\n",
      " [-7.34279169 -2.18997721 -2.32021694 -2.18997733]\n",
      " [-2.33425528 -1.7        -2.3086754  -1.7       ]\n",
      " [-9.51878055 -1.         -1.95863069 -1.        ]\n",
      " [-1.53606311  0.         -1.5458317  -0.93538918]\n",
      " [-2.03123803 -1.93922286 -1.96160406 -1.7       ]\n",
      " [-1.997358   -1.62565254 -1.92147159 -1.        ]\n",
      " [-1.50205309 -0.90152291 -1.60772148  0.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n",
      "Best Policy with Q Learning\n",
      "[[ 3.  3.  3.  1.]\n",
      " [ 7.  1.  7.  1.]\n",
      " [ 1.  3.  3.  1.]\n",
      " [ 3.  3.  3.  5.]]\n",
      "Corresponding Values for Q Learning\n",
      "[[-2.77308975 -2.53299871 -2.19       -1.7       ]\n",
      " [ 7.         -2.19        7.         -1.        ]\n",
      " [-2.18997721 -1.7        -1.          0.        ]\n",
      " [-1.7        -1.          0.          5.        ]]\n",
      "Index for actions:\n",
      "0 : up\n",
      "1 : down\n",
      "2 : left\n",
      "3 : right\n",
      "5 : terminal states (stay)\n",
      "7 : wall\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def q_learning(P,no_episodes,no_steps,alpha,discount,epsilon):\n",
    "    fails=0\n",
    "    Q=np.zeros((noS,noA))\n",
    "    \n",
    "    for e in range(no_episodes):\n",
    "        \n",
    "        #starting point\n",
    "        S=random.randrange(noS)\n",
    "        step=0\n",
    "        while S!=15 and (step<no_steps): #and (S not in wall)\n",
    "            A=e_greedy(epsilon,Q[S])\n",
    "            S_,garb,R=P[S][A]\n",
    "            A_=np.argmax(Q[S_])\n",
    "            Q[S][A]=Q[S][A]+alpha*(R+discount*Q[S_][A_]-Q[S][A])\n",
    "            S=S_\n",
    "            step+=1\n",
    "        #print step    \n",
    "        if terminal_state(S)==False:\n",
    "            fails+=1\n",
    "            \n",
    "    print fails\n",
    "    return Q\n",
    "\n",
    "#Testing\n",
    "wall=[4,6]\n",
    "P=update_P(wall)\n",
    "\n",
    "fixedObstacles=True\n",
    "result_q=q_learning(P,2000,200,0.1,0.7,0.1)\n",
    "print result_q\n",
    "\n",
    "policy=np.zeros((noS))\n",
    "bestQ=np.zeros((noS))\n",
    "\n",
    "for s in range(noS):\n",
    "    if terminal_state(s):\n",
    "        policy[s]=5\n",
    "        bestQ[s]=5\n",
    "    elif s in wall and fixedObstacles==True:\n",
    "        policy[s]=7\n",
    "        bestQ[s]=7\n",
    "    else:\n",
    "        policy[s]=np.argmax(result_q[s])\n",
    "        bestQ[s]=max(result_q[s])\n",
    "\n",
    "print 'Best Policy with Q Learning'\n",
    "print policy.reshape(4,4)\n",
    "    \n",
    "print 'Corresponding Values for Q Learning'\n",
    "print bestQ.reshape(4,4)\n",
    "\n",
    "print 'Index for actions:'\n",
    "for k,v in Action_Index.items():\n",
    "    print k,\":\",v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 8 5]\n",
      "Best policy with Value Iteration is\n",
      "[[ 7.  3.  1.  2.]\n",
      " [ 1.  7.  1.  1.]\n",
      " [ 7.  1.  1.  1.]\n",
      " [ 3.  3.  3.  5.]]\n",
      "Corresponding Value Function is\n",
      "[[ 13.     -3.439  -2.71   -3.439]\n",
      " [ -3.439  13.     -1.9    -1.   ]\n",
      " [ 13.     -1.9    -1.      0.   ]\n",
      " [ -1.9    -1.      0.      0.   ]]\n",
      "Time taken\n",
      "0.00294861117618\n"
     ]
    }
   ],
   "source": [
    "#Value Iteration\n",
    "\n",
    "def value_iteration(P,discount,threshold):\n",
    "    #Initialisation\n",
    "    value=np.zeros((noS,))\n",
    "    \n",
    "    while True:\n",
    "        new_policy=np.zeros([noS,4])\n",
    "        change=0\n",
    "        for s in S:\n",
    "            if s!=15:\n",
    "                v=value[s]\n",
    "                action_values = np.zeros(noA)\n",
    "                for a in range(noA):   \t\t# Iterating over all the actions     \n",
    "                        next_state,probability,reward = P[s][a]\n",
    "                        action_values[a] += probability*(reward + discount*value[next_state])\n",
    "                max_total = np.amax(action_values)   # taking the max reward value \n",
    "                best_a = np.argmax(action_values)\n",
    "\n",
    "                value[s]=max_total\n",
    "                new_policy[s][best_a]=1\n",
    "\n",
    "                change=max(change,np.abs(v-value[s]))\n",
    "            \n",
    "        if change < threshold:\n",
    "              break\n",
    "    \n",
    "    value[wall]=13            \n",
    "    return new_policy,value.reshape(4,4)\n",
    "\n",
    "start=time.clock()\n",
    "discount=0.9\n",
    "threshold=0.0001\n",
    "best_policy,corr_value=value_iteration(P,discount,threshold)\n",
    "end=time.clock()\n",
    "\n",
    "show_best_policy=np.zeros(noS,)\n",
    "for s,p_s in enumerate(best_policy):\n",
    "    if terminal_state(s):\n",
    "        show_best_policy[s]=5\n",
    "    elif s in wall:\n",
    "        show_best_policy[s]=7\n",
    "    else:\n",
    "        show_best_policy[s]=np.argmax(p_s)\n",
    "        \n",
    "print wall    \n",
    "print 'Best policy with Value Iteration is'\n",
    "print show_best_policy.reshape(4,4)\n",
    "print 'Corresponding Value Function is'\n",
    "print corr_value.reshape(4,4)\n",
    "print 'Time taken'\n",
    "print end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 5 4]\n",
      "[[ 5.  2.  2.  3.]\n",
      " [ 0.  0.  1.  1.]\n",
      " [ 0.  1.  1.  1.]\n",
      " [ 2.  3.  3.  5.]]\n",
      "values\n",
      "[[ 0.          9.99999999  5.98515291 -3.33333333]\n",
      " [ 9.99996013  5.97952064  3.16225205  5.97399671]\n",
      " [ 5.95677926  3.19708384  5.99987169  9.99987296]\n",
      " [-3.33333333  5.99999614 10.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "def td0(P,policy,no_episodes,alpha,discount):\n",
    "    value=np.zeros((noS,))\n",
    "    \n",
    "    for e in range(no_episodes):\n",
    "        S=random.randrange(noS)\n",
    "        step=0\n",
    "        while S!=0 and S!=15 and (step<200): # and (S not in wall) \n",
    "            A=policy[S]\n",
    "            S_,garb,R=P[S][A]\n",
    "            value[S]=value[S]+alpha*(R+discount*value[S_]-value[S])\n",
    "            S=S_\n",
    "            step+=1\n",
    "    return value\n",
    "\n",
    "temp = np.random.normal(7,3,3)\n",
    "global wall\n",
    "wall = temp.astype(int)\n",
    "update_P(wall)\n",
    "print wall\n",
    "test_policy=policy\n",
    "print policy.reshape(4,4)\n",
    "result_value=td0(P,test_policy,1000,0.1,0.7)\n",
    "    \n",
    "print 'values'\n",
    "print result_value.reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
