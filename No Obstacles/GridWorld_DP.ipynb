{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of states in grid:  16\n",
      "No. of action options in each state: 4\n",
      "Index for actions:\n",
      "0 : up\n",
      "1 : down\n",
      "2 : left\n",
      "3 : right\n",
      "5 : terminal states (stay)\n",
      "7 : wall\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "#Defining Grid\n",
    "\n",
    "#defining action variables\n",
    "up=0\n",
    "down=1\n",
    "left=2\n",
    "right=3\n",
    "\n",
    "#no. of states and no. of variables\n",
    "noS=4*4\n",
    "noA=4\n",
    "\n",
    "S=range(noS)\n",
    "\n",
    "reward =-1    #for every step\n",
    "\n",
    "terminal_state = lambda s: s==0 or s==noS-1  #first and last state terminal\n",
    "wall=[]\n",
    "\n",
    "P=dict() #transition probabilities\n",
    "\n",
    "for s in S:\n",
    "    P[s]=dict()\n",
    "    \n",
    "    if (terminal_state(s)):\n",
    "        P[s][up]=(s,1.0,0.0)   # next_state, probability, reward\n",
    "        P[s][down]=(s,1.0,0.0)\n",
    "        P[s][right]=(s,1.0,0.0)\n",
    "        P[s][left]=(s,1.0,0.0)\n",
    "    else:\n",
    "        next_s= s if(s<4) else s-4\n",
    "        if next_s in wall:\n",
    "            P[s][up]=(s,1.0,-1000.0)\n",
    "        else:\n",
    "            P[s][up]=(next_s,1.0,reward)\n",
    "        \n",
    "        next_s= s if(16-s<=4) else s+4\n",
    "        if next_s in wall:\n",
    "            P[s][down]=(s,1.0,-1000.0)\n",
    "        else:\n",
    "            P[s][down]=(next_s,1.0,reward)\n",
    "        \n",
    "        next_s= s if((s+1)%4==0) else s+1\n",
    "        if next_s in wall:\n",
    "            P[s][right]=(s,1.0,-1000.0)\n",
    "        else:\n",
    "            P[s][right]=(next_s,1.0,reward)\n",
    "        \n",
    "        next_s= s if(s%4==0) else s-1\n",
    "        if next_s in wall:\n",
    "            P[s][left]=(s,1.0,-1000.0)\n",
    "        else:\n",
    "            P[s][left]=(next_s,1.0,reward)\n",
    "        \n",
    "        \n",
    "print 'No. of states in grid: ', noS\n",
    "print 'No. of action options in each state:', noA\n",
    "Action_Index=dict()\n",
    "Action_Index[0]='up'\n",
    "Action_Index[1]='down'\n",
    "Action_Index[2]='left'\n",
    "Action_Index[3]='right'\n",
    "Action_Index[5]='terminal states (stay)'\n",
    "Action_Index[7]='wall'\n",
    "\n",
    "print 'Index for actions:'\n",
    "for k,v in Action_Index.items():\n",
    "    print k,\":\",v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Policy Evaluation\n",
    "\n",
    "def policy_evaluation(P,policy,threshold,dicount):\n",
    "    value=np.zeros((noS,))\n",
    "    while True:\n",
    "        new_value=np.zeros((noS,))\n",
    "\n",
    "        change=0\n",
    "        for s in S:\n",
    "            v=0\n",
    "            \n",
    "            for a,action_prob in enumerate(policy[s]):\n",
    "                next_state,probability,reward=P[s][a]\n",
    "                temp=probability*action_prob*(reward+discount*value[next_state])\n",
    "                v+=temp\n",
    "\n",
    "            change=max(change,np.abs(v-value[s]))\n",
    "            new_value[s]=v\n",
    "\n",
    "        if change < threshold:\n",
    "              break\n",
    "\n",
    "        value=new_value\n",
    "\n",
    "    return value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value Function for policy: all actions equiprobable:\n",
      "[[  0.         -13.99887902 -19.99833891 -21.99814115]\n",
      " [-13.99887902 -17.99853668 -19.99835003 -19.99833891]\n",
      " [-19.99833891 -19.99835003 -17.99853668 -13.99887902]\n",
      " [-21.99814115 -19.99833891 -13.99887902   0.        ]]\n"
     ]
    }
   ],
   "source": [
    "#Testing policy evaluation \n",
    "random_policy = np.ones([16, 4])/4\n",
    "\n",
    "threshold = 0.0001\n",
    "discount = 1.0\n",
    "value = np.zeros((noS,))\n",
    "\n",
    "random_policy_value=policy_evaluation(P,random_policy,threshold,discount)\n",
    "random_policy_value[wall]=13\n",
    "print 'Value Function for policy: all actions equiprobable:'\n",
    "print random_policy_value.reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best policy with Policy Iteration is\n",
      "[[ 5.  2.  2.  1.]\n",
      " [ 0.  0.  0.  1.]\n",
      " [ 0.  0.  1.  1.]\n",
      " [ 0.  3.  3.  5.]]\n",
      "Corresponding Value Function is\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "Time taken\n",
      "0.0619498827247\n"
     ]
    }
   ],
   "source": [
    "#Policy Iteration\n",
    "\n",
    "def policy_iteration(P,discount,threshold):\n",
    "    #Initialisation\n",
    "    value=np.zeros((noS,))\n",
    "    policy=np.ones([16, 4])/4\n",
    "    \n",
    "    while True:\n",
    "        #Policy evaluation\n",
    "        value=policy_evaluation(P,policy,threshold,discount)\n",
    "        \n",
    "        new_value=np.zeros((noS,))\n",
    "        new_policy=np.zeros([noS,4])\n",
    "        \n",
    "        #Policy Improvement\n",
    "        policy_stable=True\n",
    "        for s in S:\n",
    "            if s!=0 and s!=15:\n",
    "                old_action=policy[s]\n",
    "                action_values = np.zeros(noA)\n",
    "                for a in range(noA):   \t\t# Iterating over all the actions     \n",
    "                        next_state,probability,reward = P[s][a]\n",
    "                        action_values[a] += probability*(reward + discount*value[next_state])\n",
    "                max_total = np.amax(action_values)   # taking the max reward value \n",
    "                best_a = np.argmax(action_values)\n",
    "\n",
    "                new_policy[s][best_a]=1\n",
    "\n",
    "                new_value[s]=max_total \n",
    "                if (np.array_equal(old_action,new_policy[s])!=True):\n",
    "                    policy_stable=False\n",
    "                    \n",
    "        value=new_value\n",
    "        \n",
    "        if policy_stable:\n",
    "            value[wall]=13\n",
    "            return new_policy,value\n",
    "        else:\n",
    "            policy=new_policy \n",
    "\n",
    "start=time.clock()\n",
    "best_policy,corr_value=policy_iteration(P,discount,threshold)\n",
    "end=time.clock()\n",
    "\n",
    "show_best_policy=np.zeros(noS,)\n",
    "for s,p_s in enumerate(best_policy):\n",
    "    if terminal_state(s):\n",
    "        show_best_policy[s]=5\n",
    "    elif s in wall:\n",
    "        show_best_policy[s]=7\n",
    "    else:\n",
    "        show_best_policy[s]=np.argmax(p_s)\n",
    "\n",
    "    \n",
    "print 'Best policy with Policy Iteration is'\n",
    "print show_best_policy.reshape(4,4)\n",
    "print 'Corresponding Value Function is'\n",
    "print corr_value.reshape(4,4)\n",
    "print 'Time taken'\n",
    "print end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-3bbdabd41e01>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_policy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[0mstart\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[0mbest_policy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcorr_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mP\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdiscount\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mthreshold\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclock\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "#Value Iteration\n",
    "\n",
    "def value_iteration(P,discount,threshold):\n",
    "    #Initialisation\n",
    "    value=np.zeros((noS,))\n",
    "    \n",
    "    while True:\n",
    "        new_policy=np.zeros([noS,4])\n",
    "        change=0\n",
    "        for s in S:\n",
    "            if s!=0 and s!=15:\n",
    "                v=value[s]\n",
    "                action_values = np.zeros(noA)\n",
    "                for a in range(noA):   \t\t# Iterating over all the actions     \n",
    "                        next_state,probability,reward = P[s][a]\n",
    "                        action_values[a] += probability*(reward + discount*value[next_state])\n",
    "                max_total = np.amax(action_values)   # taking the max reward value \n",
    "                best_a = np.argmax(action_values)\n",
    "\n",
    "                value[s]=max_total\n",
    "                new_policy[s][best_a]=1\n",
    "\n",
    "                change=max(change,np.abs(v-value[s]))\n",
    "            \n",
    "        if change < threshold:\n",
    "              break\n",
    "    \n",
    "    value[wall]=13            \n",
    "    return new_policy,value.reshape(4,4)\n",
    "\n",
    "start=time.clock()\n",
    "best_policy,corr_value=value_iteration(P,discount,threshold)\n",
    "end=time.clock()\n",
    "\n",
    "show_best_policy=np.zeros(noS,)\n",
    "for s,p_s in enumerate(best_policy):\n",
    "    if terminal_state(s):\n",
    "        show_best_policy[s]=5\n",
    "    elif s in wall:\n",
    "        show_best_policy[s]=7\n",
    "    else:\n",
    "        show_best_policy[s]=np.argmax(p_s)\n",
    "    \n",
    "print 'Best policy with Value Iteration is'\n",
    "print show_best_policy.reshape(4,4)\n",
    "print 'Corresponding Value Function is'\n",
    "print corr_value.reshape(4,4)\n",
    "print 'Time taken'\n",
    "print end-start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our Value Function:\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n"
     ]
    }
   ],
   "source": [
    "print 'Our Value Function:'\n",
    "print corr_value.reshape(4,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
